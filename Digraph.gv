digraph {
	graph [size="34.199999999999996,34.199999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140532712050576 [label="
 ()" fillcolor=darkolivegreen1]
	140532712352160 [label="AddBackward0
------------
alpha: 1"]
	140532712349856 -> 140532712352160
	140532712349856 -> 140532695448688 [dir=none]
	140532695448688 [label="other
 ()" fillcolor=orange]
	140532712349856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140532712352592 -> 140532712349856
	140532712352592 [label="SumBackward0
----------------------------
self_sym_sizes: (1990, 1, 3)"]
	140532712350480 -> 140532712352592
	140532712350480 -> 140532695448528 [dir=none]
	140532695448528 [label="result
 (1990, 1, 3)" fillcolor=orange]
	140532712350480 [label="SqrtBackward0
----------------------
result: [saved tensor]"]
	140532712351104 -> 140532712350480
	140532712351104 [label="AddBackward0
------------
alpha: 1"]
	140532712351632 -> 140532712351104
	140532712351632 -> 140532712044480 [dir=none]
	140532712044480 [label="self
 (1990, 1, 3)" fillcolor=orange]
	140532712351632 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140532712352208 -> 140532712351632
	140532712352208 [label="SubBackward0
------------
alpha: 1"]
	140532712353264 -> 140532712352208
	140532712353264 -> 140532712074432 [dir=none]
	140532712074432 [label="indices[0]
 (1990, 1)" fillcolor=orange]
	140532712353264 [label="IndexBackward0
-------------------------------
indices       : [saved tensors]
self_sym_sizes:       (2048, 3)"]
	140532712350000 -> 140532712353264
	140532712350000 [label="SumBackward1
---------------------------------------
dim           : (18446744073709551614,)
keepdim       :                   False
self_sym_sizes:         (2048, 1024, 3)"]
	140532712351440 -> 140532712350000
	140532712351440 -> 140532695471104 [dir=none]
	140532695471104 [label="other
 (2048, 1024, 3)" fillcolor=orange]
	140532712351440 -> 140532695472464 [dir=none]
	140532695472464 [label="self
 (2048, 1024, 1)" fillcolor=orange]
	140532712351440 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140532712350144 -> 140532712351440
	140532712350144 -> 140532695472704 [dir=none]
	140532695472704 [label="self
 (2048, 1024, 1)" fillcolor=orange]
	140532712350144 [label="NanToNumBackward0
--------------------
self: [saved tensor]"]
	140532712350384 -> 140532712350144
	140532712350384 -> 140532695472864 [dir=none]
	140532695472864 [label="other
 (2048, 1024, 1)" fillcolor=orange]
	140532712350384 -> 140532695473024 [dir=none]
	140532695473024 [label="self
 (2048, 1024, 1)" fillcolor=orange]
	140532712350384 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140532712353408 -> 140532712350384
	140532712353408 [label="RsubBackward1
-------------
alpha: 1"]
	140532712352256 -> 140532712353408
	140532712352256 -> 140532695448928 [dir=none]
	140532695448928 [label="result
 (2048, 1024, 1)" fillcolor=orange]
	140532712352256 [label="ExpBackward0
----------------------
result: [saved tensor]"]
	140532712349952 -> 140532712352256
	140532712349952 [label=NegBackward0]
	140532712351200 -> 140532712349952
	140532712351200 -> 140532712043280 [dir=none]
	140532712043280 [label="self
 (2048, 1024, 1)" fillcolor=orange]
	140532712351200 [label="MulBackward0
---------------------
other:           None
self : [saved tensor]"]
	140532712353216 -> 140532712351200
	140532712353216 -> 140532695605312 [dir=none]
	140532695605312 [label="other
 (2048, 1024, 1)" fillcolor=orange]
	140532712353216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140532699735760 -> 140532712353216
	140532699735760 [label=_TruncExpBackward]
	140532718141696 -> 140532699735760
	140532718141696 [label="AddBackward0
------------
alpha: 1"]
	140532718145056 -> 140532718141696
	140532718145056 [label="SplitWithSizesBackward0
------------------------------------
dim           : 18446744073709551615
self_sym_sizes:     (2048, 1024, 16)
split_sizes   :              (1, 15)"]
	140532718144144 -> 140532718145056
	140532718144144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (2097152, 16)"]
	140532718145200 -> 140532718144144
	140532718145200 -> 140532695609152 [dir=none]
	140532695609152 [label="indices[0]
 (2097152)" fillcolor=orange]
	140532718145200 [label="IndexPutBackward0
---------------------------
accumulate:           False
indices   : [saved tensors]"]
	140532718145344 -> 140532718145200
	140532718145344 [label="SliceBackward0
-----------------------------
dim           :             0
end           :       2051691
self_sym_sizes: (2051840, 16)
start         :             0
step          :             1"]
	140532699735520 -> 140532718145344
	140532699735520 [label=_module_functionBackward]
	140532718144240 -> 140532699735520
	140532718144240 [label="ConstantPadNdBackward0
----------------------
pad: (0, 0, 0, 149)"]
	140532718144528 -> 140532718144240
	140532718144528 [label=CppFunction]
	140532718143712 -> 140532718144528
	140537589629472 [label="
 (33554432, 2)" fillcolor=lightblue]
	140537589629472 -> 140532718143712
	140532718143712 [label=AccumulateGrad]
	140532718144048 -> 140532699735520
	140532718144048 [label=ToCopyBackward0]
	140532718144096 -> 140532718144048
	140532712071872 [label="field.base_network.params
 (6144)" fillcolor=lightblue]
	140532712071872 -> 140532718144096
	140532718144096 [label=AccumulateGrad]
	140532695606352 -> 140532699735520
	140532695606352 [label="
 (2051840, 32)" fillcolor=orange]
	140532695606912 -> 140532699735520
	140532695606912 [label="
 (6144)" fillcolor=orange]
	140532695448368 -> 140532699735520
	140532695448368 [label="
 (2051840, 16)" fillcolor=orange]
	140532695607792 -> 140532699735760
	140532695607792 [label="
 (2048, 1024, 1)" fillcolor=orange]
	140532712350960 -> 140532712350384
	140532712350960 -> 140532695448368 [dir=none]
	140532695448368 [label="result
 (2048, 1024, 1)" fillcolor=orange]
	140532712350960 [label="ExpBackward0
----------------------
result: [saved tensor]"]
	140532712352688 -> 140532712350960
	140532712352688 [label=NegBackward0]
	140532712353504 -> 140532712352688
	140532712353504 [label="CatBackward0
-------------------------
dim: 18446744073709551614"]
	140532718145104 -> 140532712353504
	140532718145104 [label="CumsumBackward0
-------------------------
dim: 18446744073709551614"]
	140532718145488 -> 140532718145104
	140532718145488 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:     (2048, 1023, 1)
start         :                   0
step          :                   1"]
	140532718141504 -> 140532718145488
	140532718141504 [label="SliceBackward0
------------------------------------
dim           :                    1
end           : 18446744073709551615
self_sym_sizes:      (2048, 1024, 1)
start         :                    0
step          :                    1"]
	140532712351200 -> 140532718141504
	140532712351872 -> 140532712351440
	140532712351872 [label=ToCopyBackward0]
	140532712352976 -> 140532712351872
	140532712352976 [label="ViewBackward0
----------------------------
self_sym_sizes: (2097152, 3)"]
	140532712352112 -> 140532712352976
	140532712352112 [label="SliceBackward0
-----------------------------
dim           :             1
end           :             3
self_sym_sizes: (2097152, 16)
start         :             0
step          :             1"]
	140532699734080 -> 140532712352112
	140532699734080 [label=_module_functionBackward]
	140532718144480 -> 140532699734080
	140532718144480 [label="CatBackward0
-------------------------
dim: 18446744073709551615"]
	140532718145296 -> 140532718144480
	140532718145296 [label=AliasBackward0]
	140532699734560 -> 140532718145296
	140532699734560 [label=_module_functionBackward]
	140532718144000 -> 140532699734560
	140532718144000 [label=ToCopyBackward0]
	140532718141984 -> 140532718144000
	140532712074032 [label="field.direction_encoding.params
 (0)" fillcolor=lightblue]
	140532712074032 -> 140532718141984
	140532718141984 [label=AccumulateGrad]
	140532695471504 -> 140532699734560
	140532695471504 [label="
 (2097152, 3)" fillcolor=orange]
	140532695471824 -> 140532699734560
	140532695471824 [label="
 (0)" fillcolor=orange]
	140532695448448 -> 140532699734560
	140532695448448 [label="
 (2097152, 16)" fillcolor=orange]
	140532718144912 -> 140532718144480
	140532718144912 [label="ViewBackward0
--------------------------------
self_sym_sizes: (2048, 1024, 15)"]
	140532718145056 -> 140532718144912
	140532718143664 -> 140532718144480
	140532718143664 [label="ViewBackward0
--------------------------------
self_sym_sizes: (2048, 1024, 32)"]
	140532718144192 -> 140532718143664
	140532718144192 -> 140532712072672 [dir=none]
	140532712072672 [label="indices
 (2048, 1024)" fillcolor=orange]
	140532718144192 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  704"]
	140532718143376 -> 140532718144192
	140532712071392 [label="field.embedding_appearance.embedding.weight
 (704, 32)" fillcolor=lightblue]
	140532712071392 -> 140532718143376
	140532718143376 [label=AccumulateGrad]
	140532718142512 -> 140532699734080
	140532718142512 [label=ToCopyBackward0]
	140532718141600 -> 140532718142512
	140532712046480 [label="field.mlp_head.params
 (26624)" fillcolor=lightblue]
	140532712046480 -> 140532718141600
	140532718141600 [label=AccumulateGrad]
	140532695473424 -> 140532699734080
	140532695473424 [label="
 (2097152, 63)" fillcolor=orange]
	140532695472784 -> 140532699734080
	140532695472784 [label="
 (26624)" fillcolor=orange]
	140532695446688 -> 140532699734080
	140532695446688 [label="
 (2097152, 16)" fillcolor=orange]
	140532712353024 -> 140532712352160
	140532712353024 -> 140532695446688 [dir=none]
	140532695446688 [label="other
 ()" fillcolor=orange]
	140532712353024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140532712350576 -> 140532712353024
	140532712350576 -> 140532695449008 [dir=none]
	140532695449008 [label="self
 (2048, 2)" fillcolor=orange]
	140532712350576 -> 140532712074832 [dir=none]
	140532712074832 [label="target
 (2048)" fillcolor=orange]
	140532712350576 -> 140532695448448 [dir=none]
	140532695448448 [label="total_weight
 ()" fillcolor=orange]
	140532712350576 [label="NllLossBackward0
----------------------------------
ignore_index: 18446744073709551516
reduction   :                    1
self        :       [saved tensor]
target      :       [saved tensor]
total_weight:       [saved tensor]
weight      :                 None"]
	140532712351728 -> 140532712350576
	140532712351728 -> 140532695449408 [dir=none]
	140532695449408 [label="result
 (2048, 2)" fillcolor=orange]
	140532712351728 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140532712351392 -> 140532712351728
	140532712351392 [label="SumBackward1
---------------------------------------
dim           : (18446744073709551614,)
keepdim       :                   False
self_sym_sizes:         (2048, 1024, 2)"]
	140532712353312 -> 140532712351392
	140532712353312 -> 140532695473824 [dir=none]
	140532695473824 [label="other
 (2048, 1024, 2)" fillcolor=orange]
	140532712353312 -> 140532695472464 [dir=none]
	140532695472464 [label="self
 (2048, 1024, 1)" fillcolor=orange]
	140532712353312 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140532712350144 -> 140532712353312
	140532712351008 -> 140532712353312
	140532712351008 [label="ViewBackward0
----------------------------
self_sym_sizes: (2097152, 2)"]
	140532718143808 -> 140532712351008
	140532718143808 -> 140532695449248 [dir=none]
	140532695449248 [label="mat1
 (2097152, 128)" fillcolor=orange]
	140532718143808 -> 140532695447408 [dir=none]
	140532695447408 [label="mat2
 (128, 2)" fillcolor=orange]
	140532718143808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  : (2097152, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 2)
mat2_sym_strides:       (1, 128)"]
	140532718144864 -> 140532718143808
	140532712045360 [label="field.field_head_semantics.net.bias
 (2)" fillcolor=lightblue]
	140532712045360 -> 140532718144864
	140532718144864 [label=AccumulateGrad]
	140532718142080 -> 140532718143808
	140532718142080 [label="ViewBackward0
---------------------------------
self_sym_sizes: (2048, 1024, 128)"]
	140532718143856 -> 140532718142080
	140532718143856 [label=ToCopyBackward0]
	140532718142464 -> 140532718143856
	140532718142464 [label="ViewBackward0
------------------------------
self_sym_sizes: (2097152, 128)"]
	140532718142272 -> 140532718142464
	140532718142272 [label=AliasBackward0]
	140532713479760 -> 140532718142272
	140532713479760 [label=_module_functionBackward]
	140532718142848 -> 140532713479760
	140532718142848 [label=ToCopyBackward0]
	140532718142320 -> 140532718142848
	140532712045760 [label="field.mlp_semantics.params
 (9216)" fillcolor=lightblue]
	140532712045760 -> 140532718142320
	140532718142320 [label=AccumulateGrad]
	140532695471024 -> 140532713479760
	140532695471024 [label="
 (2097152, 15)" fillcolor=orange]
	140532695473184 -> 140532713479760
	140532695473184 [label="
 (9216)" fillcolor=orange]
	140532695448128 -> 140532713479760
	140532695448128 [label="
 (2097152, 128)" fillcolor=orange]
	140532718143760 -> 140532718143808
	140532718143760 [label=TBackward0]
	140532718142560 -> 140532718143760
	140532712045680 [label="field.field_head_semantics.net.weight
 (2, 128)" fillcolor=lightblue]
	140532712045680 -> 140532718142560
	140532718142560 [label=AccumulateGrad]
	140532712352160 -> 140532712050576
}
